{"nbformat_minor": 2, "cells": [{"source": "# Policy Making in Malaria with AI (beta)\n\nIn most areas of public policy and decision making, it\u2019s difficult to make evidence based decisions because of the difficulty involved in exploring options for truly complex decisions. Policy making in malaria is a good example of these complex decisions. In this notebook, we explore policy making with malaria by considering only two interventions space i.e. spraying and using bed nets with an open-malaria model for Rachuonyo South in Western Kenya. A reward function based on cost-effectiveness per Daly averted is used. \n\nThis notebook is structured as follows:\n\n 1. **Running the experiments with random actions** : In this section, we run several experiments with random actions and visualize the outcome reward by creating some MatPlotLib visualisations\n 2. **Implementing Machine Learning and AI on models** : We implement a Genetic Algorithm Model after which we look at reward visualization with MatPlotLib", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "!pip install git+https://github.com/slremy/estool git+https://github.com/slremy/netsapi --user --upgrade\n%load_ext autoreload\n%autoreload 2    \n\nimport numpy as np\n\nfrom netsapi.challenge import *\nfrom netsapi.visualisation import *\nfrom es import SimpleGA, RemyGA\n\nfrom sys import exit, exc_info, argv\n\nuserId = None"}, {"source": "# 1. Running the experiments with random actions\n", "cell_type": "markdown", "metadata": {}}, {"source": "**Running our first experiment!**\n\nLet's start with initializing our experiment to generate our experiment id. Then we run a simulation describing a current intervention campaign 55% ITN and 70% IRS coverage and obtain our reward.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "source": "actions = np.array([[0.55, 0.70]])\nreward = evaluate(actions)\nsolution = np.append(actions.T, reward)\nprint(solution)"}, {"source": "**Running more experiments with different types of actions**\n\nLet's run experiments with 4 sets of actions [ITN, IRS] and visualise the rewards as a simple scatter of points as a response surface (MatPlotLib may be required but you are free to visualise the data how you see fit)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "output = []\nactions = np.array([[0.55, 0.70]])\nreward = evaluate(actions)\nsolution = np.append(actions.T, reward)\nprint(solution)\noutput.append(solution)"}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "actions = np.array([[0.68, 0.70]])\nreward = evaluate(actions)\nsolution = np.append(actions.T, reward)\nprint(solution)\noutput.append(solution)"}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "actions = np.array([[0.78, 0.70]])\nreward = evaluate(actions)\nsolution = np.append(actions.T, reward)\nprint(solution)\noutput.append(solution)"}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "actions = np.array([[0.88, 0.70]])\nreward = evaluate(actions)\nsolution = np.append(actions.T, reward)\nprint(solution)\noutput.append(solution)"}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "output = np.array(output)\nprint(output)"}, {"execution_count": null, "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "source": "actions = np.empty(shape=(10,2)) #10 actions [ITN, IRS]\nactions[:,0] = np.random.rand(10) \nactions[:,1] = .70               #don't change this yet!\nrewards = np.array(evaluate(actions))"}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "output = np.hstack((actions,rewards.reshape(-1,1)))\nprint(output)"}, {"source": "**Visualising rewards**", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "source": "ResponseSurface(output)"}, {"source": "# 2. Implementing Machine Learning and AI on models\n\n**Genetic Algorithm**\n\nImplemented in ESTool (Ha, David. \"Neuroevolution for deep reinforcement learning problems.\" Proceedings of the Genetic and Evolutionary Computation Conference Companion. ACM, 2018.)\n\n\nWe use a parameter size of 2, a population size of 20 and consider 10 generations. For each generation we candidate policies to evaluate based on evolutionary pressure, calculate the reward and create a reward vector. We save all the results in a 3 dimensional vector space and aggregate for all generations.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "source": "num_paramters = 2\n\ndef mutate(chromosome):\n    mutation_rate = .1\n    for j in range(num_paramters):\n        r = np.random.rand(1);\n        if(r > mutation_rate):\n            chromosome[j] = np.remainder(chromosome[j]+np.random.randn(1),0.99);\n    return chromosome\n\ndef make_random_individuals(x,y):\n    value=np.random.rand(x,y);\n    return value\n\n\ndef boundary(individual):\n    processed = np.clip(individual,0,1)\n    return processed\n\n\nsolver = RemyGA(num_paramters,                # number of model parameters\n            random_individuals_fcn=make_random_individuals,\n            mutate_fcn = mutate,\n            sigma_init=1,        # initial standard deviation\n            popsize=20,   # population size\n            elite_ratio=0.3,       # percentage of the elites\n            forget_best=False,     # forget the historical best elites\n            weight_decay=0.00,     # weight decay coefficient\n             )\n\nnum_generations = 10\nhistory0=  np.empty(shape=(num_generations, solver.popsize, solver.num_params+1))\nfor i in range(num_generations):\n    try:\n        # ask for a set of candidate solutions to be evaluated\n        solutions = solver.ask(boundary)\n        # calculate the reward for each given solution using our own method\n        rewards = evaluate(solutions)\n        solver.tell(rewards)\n\n        # get best parameter, reward from ES\n        reward_vector = solver.result()\n        print(reward_vector[0],reward_vector[1],i, num_generations)\n        history0[i,:,:] = np.hstack((solutions,np.array(rewards).reshape(-1,1)))\n    except (KeyboardInterrupt, SystemExit):\n        print(exc_info())"}, {"source": "**Visualization of reward vectors**\n\nLet's investigate the shape of our result vector", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "history0.shape"}, {"source": "Let's reshape our result vector so as to visualize the results for all of the 10 generations in one plot", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "ResponseSurface(history0.reshape(200,3))"}, {"source": "And finally we are going to visialize the 10 generations in a box plot.\n\nRemember, as you \"evolve\" your solutions, you're looking for the best. \n\nDo you see any evidence of that in this plot?", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "source": "plt.boxplot([i[:,-1] for i in history0]);"}, {"source": "## Now time to try and break it! ", "cell_type": "markdown", "metadata": {}}, {"source": "You're now free to explore! Given the code snippets of how to post single actions.\nSeveral methods, approaches or algorithms may stand to benefit the uses of high bandwidth computation for machine learning.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "source": "popsize=10\nnum_real_paramters = 2\nwidth = 10\nnum_paramters = num_real_paramters * width\nnum_generations = 10\n\n#converts from binary representation to the actual action\ndef RealAction(action):\n    realaction=()\n    for i in range(int(num_paramters/width)):\n        a1 = np.array(action[i*width:(i+1)*width])\n        realaction = realaction + (a1.dot(2**np.arange(a1.size)[::-1])/pow(2,width),)\n    realaction = np.round(realaction,3);\n    return realaction\n\ndef mutate(chromosome):\n    mutation_rate = .2\n    for j in range(chromosome.shape[0]):\n        r = np.random.rand(1);\n        if(r > mutation_rate):\n            chromosome[j] = int(np.invert(bool(chromosome[j])));\n    return chromosome\n\ndef make_random_individuals(x,y):\n    value=np.random.choice([0, 1], size=(x,y));\n    return value\n\ndef boundary(individual):\n    processed = individual%(1+np.finfo(float).eps)\n    return processed\n\nsolver = RemyGA(num_paramters,         # number of model parameters\n                random_individuals_fcn=make_random_individuals,\n                mutate_fcn = mutate,\n                sigma_init=1,          # initial standard deviation\n                popsize=popsize,       # population size\n                elite_ratio=0.3,       # percentage of the elites\n                forget_best=False,     # forget the historical best elites\n                weight_decay=0.00,     # weight decay coefficient\n                )\n\nhistory1=  np.empty(shape=(num_generations, popsize, num_real_paramters+1))\nfor i in range(num_generations):\n    try:\n        # ask for a set of random candidate solutions to be evaluated\n        solutions = solver.ask(boundary)\n        # calculate the reward for each given solution using our own method\n        newsolutions = []\n        for soln in solutions:\n            newsolutions.append(RealAction(soln))\n        newsolutions = np.array(newsolutions)\n        rewards = evaluate(newsolutions)\n        solver.tell(rewards)\n        \n        # get best parameter, reward from ES\n        reward_vector = solver.result()\n        print(RealAction(reward_vector[0]), reward_vector[1], i, num_generations)\n        history1[i,:,:] = np.hstack((newsolutions,np.array(rewards).reshape(-1,1)))\n    except (KeyboardInterrupt, SystemExit):\n        print(exc_info())"}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "newdata=history1.reshape(num_generations*popsize,num_real_paramters+1)"}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": "ResponseSurface(newdata)"}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}